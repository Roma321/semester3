### Методы, используемые в экспериментах

В ходе исследования была разработана методология для обнаружения грамматических ошибок, с акцентом на некорректное употребление существительных с предлогами. Основным методом анализа стало использование статистического подхода, основанного на частоте взаимной встречаемости слов, предлогов и грамматических падежей. 

1. Сбор корпуса данных: был создан корпус частоты взаимного употребления, содержащий 3.2 млн записей, а также корпус размера 101 000 для классификации грамматически корректных и некорректных предложений. Для создания корпуса взаимного употребления использовался инструмент Stanza, а также несколько ранее собранных корпусов: корпус аннотаций на медиа-платформе "Кинопописк", корпус статей русскоязычной Википедии и несколько художественных произведенй Д. Лондона и Х. К. Андерсена. Неточности работы Stanza компенсируются объёмом корпуса. 

2. Алгоритмы: Построены алгоритмы для детекции ошибок употребления существительных  с предлогом на основе статистических характеристик. Пример кода:
    ```python
    def is_prep_not_common_for_case(prep, case):
        prep_percentage_all = get_prep_percentage_full(prep)
        prep_percentage_for_case = get_prep_percentage_for_case(prep, case)
        return prep_percentage_for_case * 3 < prep_percentage_all

    def get_prep_percentage_for_case(prep, case):
        query = """SELECT 
            (COUNT(*) FILTER (WHERE preposition = %s) * 100.0 / COUNT(*)) AS percentage
        FROM 
            public.phrases where dep_case = %s;"""
        cursor.execute(query, (prep, case))
        data = cursor.fetchone()
        return float(data[0])

    def get_prep_percentage_full(prep):
        query = """SELECT 
        (COUNT(*) FILTER (WHERE preposition = %s) * 100.0 / COUNT(*)) AS percentage
    FROM 
        public.phrases;"""
        cursor.execute(query, (prep,))
        data = cursor.fetchone()
        return float(data[0])
    ```
    **Основная функция `is_prep_not_common_for_case(prep, case)`**:
    - Она проверяет, является ли предлог `prep` редким для конкретного падежа `case`.
    - Для этого она вычисляет процент использования предлога `prep` в контексте указанного падежа и сравнивает его с общим процентом использования этого предлога по всем случаям.
    - Если процент использования предлога в указанном падеже (умноженный на 3) меньше, чем общий процент, функция возвращает `True`, что говорит о том, что предлог не является обычным для данного падежа.

    Иными словами, проверяется, с каким предлогами обычно употребляется указанный грамматический падеж, и если он употребляется с указанным предлогом значительно реже, чем данный предллог встречается в текстах в принципе, то вероятно данный предлог (или падеж) был употреблён некорректно.

    Остальные методы выполняют похожие проверки на взаимное употребление предлога с главным членом словосочетания, предлога с зависимым и т. д.

3. Начато обучение нейросети BERT для бинарной классификации текстов на корректные и некорректные. Обучение проводилось на 1000 образцах из каждого класса, с 10 эпохами.

4. Полуавтоматический подход: Для задачи детекции некорректного употребления предлогов был использован полуавтоматический способ создания корпуса, включающий случайные замены предлогов или падежей зависимых существительных. В том числе были оставлены грамматически корректные, но очевидно неверные словосочетания (н-р, "смотреть через высоту" вместо "смотреть в  высоту"), такие случаи тоже интерпретировались как ошибка. 

### Результаты экспериментов

1. Анализ показал, что разработанный метод для детекции некорректного употребления существительных с предлогом достиг 74% точности в идентификации ошибок на тестовом наборе, с ложноположительным срабатыванием на уровне 18%.

2. Обучение модели BERT показало положительную динамику в снижении значения функции потерь (loss) с 0.68 до 0.49 после 10 эпох, промежуточная F1-мера составила 0.59. Продолжение обучения имело перспективу дальнейшего снижения функции потери, но было остановлено ввиду нехватки ресурсов и времени.

Матрица ошибок представлена ниже 

|         | Pred.Cor | Pred. not Cor |
|---------|----------|---------------|
| Cor.    | 198      | 3             |
| not Cor | 141      | 58            |

Стоит отметить, что практически отсутствуют "признания" грамматически корректных предложений некорректными.

### Интерпретация полученных результатов

Результаты показывают, что выбранные методы детекции грамматических ошибок эффективно выявляют ошибки в употреблении существительных с предлогами, хотя существует определенный уровень ложноположительных срабатываний, что требует дальнейшей доработки алгоритма. Успешное обучение модели BERT указывает на потенциал использования методов машинного обучения для улучшения точности выявления грамматических ошибок.

### Краткая сводка новых результатов за этот семестр

В этом семестре были достигнуты следующие результаты:
- Разработан и протестирован метод для детекции некорректного употребления существительных с предлогами.
- Получены данные по обучению нейросети BERT, подтвердившие возможность использования данного подхода для классификации грамматически корректных и некорректных текстов.
- Составлен корпус для задачи детекции некорректного употребления предлогов в полуавтоматическом режиме, что позволило провести первичный анализ и оценить точность метода. 

В целом, работа продвигается в направлении создания более точной и эффективной системы для выявления грамматических ошибок в текстах, что является актуальной задачей для изучающих русский язык как иностранный.